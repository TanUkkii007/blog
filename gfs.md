# The Google File System


## Abstract

負荷を計測するにつれ、伝統的な分散ファイルシステムとはかなり異なるデザインになった

## 1. INTRODUCTION

今と将来の予測に基づくアプリケーションの負荷と技術的環境によって、ファイルシステムのデザインは変わっていった。

- コンポーネントの障害は例外ではなく通常
- 巨大なファイル。数Gbが普通。全データでTbあるデータを数Kb単位で１０億管理するのは難しい
- 書き込みは追記。ランダム書き込みはしない。
- アプリケーションとファイルシステムAPIを一緒にデザインすることで、柔軟性やシンプルさを得る

複数のクラスター
大きい物は１０００台のノードで300Tb
１００台のクライアントアクセス


## 2. DESIGN OVERVIEW

### 2.1 Assumptions

仮定

- 常に障害の可能性をもつコモディティーハードウェア
- 適度な数の巨大なファイル。100Mbぐらいのファイル数百万。数Gbは普通。
- 負荷は主に２つのRead：大きなストリームReadと小さなランダムRead。
- 負荷は大きなシーケンシャルWriteもある。一度書いたファイルはほとんど変更されない。
- 並列に同じファイルに追記書き込みを行う複数のクライアントのよく定義されたセマンティックスを効率よく実装。ファイルは多様な方法でマージ可能なproduce-consumerキューとして使われる。数百のproducerをさばける。最小の同期オーバーヘッドのアトミック性が重要。consumerはファイルを後で読むか、同時に読み込む。
- 高い帯域は低いレイテンシよりも重要。我々のアプリケーションはデータをバルクで処理する

### 2.2 Interface

- reate
- delete
- open
- close
- read
- write
- snapshot
- record append (アトミック性を保ちながら複数のクライアントが同じファイルに追記する操作。multi-way mergeやproduce-consumerキューを実装するときに使う)

### 2.3 Architecture

１つのmasterと複数のchunkserverから成る。

ファイルは固定長のchunkに分割される。chunkはイミュータブルでグローバルに一意な64bitのchunk handleで識別される。ChunkserverはchunkをLinuxファイルとしてローカルのディスクに保存し、chunk handleとbyte rangeを与えられて読み書きする。chunkは複数のchunkserverにレプリケートされる。

masterはすべてのファイルシステムのメタデータを管理する。たとえばnamespace, access control, mapping from files to chunks, current location of chunksを管理する。
chunk lease management, 迷子chunkのガーベッジコレクション、chunkserver間のchunkのマイグレーションなどシステムワイドなアクティビティも管理する。masterはchunkserverと定期的にHeartBeatメッセージを送って命令を送ったり状態を収集したりする。

クライアントはメタデータ操作はmasterとやりとりするが、データは直接chunkserverとやりとりする。POSIX APIは実装していないのでLinuxのvnodeレイヤーはフックする必要がない。

クライアントもchunkserverもファイルデータをキャッシュしない。クライアントやシステムのキャッシュを実装しないことによって、キャッシュのコヒーレンス問題を考える必要がなく単純化できる（ただしクライアントはメタデータをキャッシュしている）。chunkはローカルのファイルとして保存されており、アクセス頻度の高いものはLinuxのバッファーキャッシュでメモリーに保持されるので、chunkserverはキャッシュを実装する必要はない。

### 2.4 Single Master

single master をもつことはchunkの配置やレプリケーションの決定を単純にする。しかしボトルネックにならないようreadとwriteへの関与を最小化しなければならない。クライアントはmasterからファイルを読み書きしない。その代わりクライアントはどのchunkserverとコンタクトをとるか尋ねる。クライアントは情報を一定期間キャッシュし直接chunkserverを操作する。

Figure 1を説明する。クライアントは固定長のchunkサイズを使い、ファイル名とバイトオフセットをファイルのchunkインデックスに変換する。次にmasterにファイル名とchunk indexを含んだリクエストを送る。masterはchunk handleとレプリカの場所を返す。クライアントはこの情報をキャッシュしファイル名とインデックスをキーにする。クライアントは（最も近いだろう）レプリカの１つにリクエストを送る。リクエストはchunk handleとchunk中のbyte範囲を指定する。同一のchunkの後続の読み込みはキャッシュが切れるかファイルが再度openにならないかぎりmasterとのやりとりは必要ない。実際にはクライアントはおなじリクエストで複数のchunkを要求するから、masterは後続のchunkの情報を含めて返す。この追加情報は将来のmasterとのやりとりのコストを下げる。


### 2.5 Chunk Size

- ブロックサイズは64 Mb。通常のファイルシステムよりはるかに大きい

メリット
1. clientのmasterとのインタラクションを減らせる。最初にchunkの位置を尋ねるだけでよい。シーケンシャル書き込みのときは特に効果がある。ランダム読み込みのときでも数Tbアクセスするために必要なchunkの位置はすべてclientのキャッシュにおさまる
2. クライアントは取得した大きなデータの処理にしばらくかかるので、永続コネクションを開放できる
3. masterサーバーのメタデータのサイズを抑えることができる。これによりすべてのメタデータがmasterサーバーのメモリにおさまる

デメリット
1. 数chunkしかない小さなファイルを保存してしまうとホットスポットになる。Googleではこのような使い方はしない。ただしバッチジョブの実行ファイルを保存して多くのサーバーが読み込んだ時にこの問題はおきた。問題を回避するためにreplication factorを上げる必要があった


### 2.6 Metadata

masterサーバーは３つのメタデータを保持する
1. ファイルとchunkの名前空間
2. ファイルからchunkへのマッピング
3. chunkのレプリカの場所

名前空間とファイルからchunkへのマッピングへの変更はディスクにoperation log として永続化される。これによってmasterがクラッシュしても不整合な状態にならない。

masterはchunkの場所を永続化しない。その代わり起動時やchunkserverがクラスターに参加したときにchunkserevrに尋ねる。


#### 2.6.1 In-Memory Data Structures

メタデータはmasterのメモリーにあるため、masterの操作は高速。

定期スキャンも高速
- chunk garbage collection
- chunkserver障害時のre-replication
- リバランスのためのchunkのマイグレーション

メタデータはmasterのメモリーにあるため、クラスターのキャパシティはmasterのメモリー量に制限される

しかし大した制限ではない
- 64Mb chunkにつき64byteのメタデータ
- ファイルの名前空間はファイルにつき64byte以下


#### 2.6.2 Chunk Locations

masterはどのchunkserverがchunkをもっているかという情報を永続化しない。それは起動時にchunkserverから取得する。その後はmasterがchunkの場所を決定できる、かつchunkserverのHeartBeatを監視しているので、状態を完全に同期できる。

永続化しないことによって、chunkserverがクラスターに参加したり去ったり名前が変更されたり障害が起きたり再起動した時に、masterが状態を同期するさいの問題を消しされる。数百台のノードがあるとこれらは頻繁におきる。

masterではなくchunkserverが自身のディスク上のchunkの情報に責任をもつことにより、chunkserevrのディスク障害時やオペレーターが名前を変更したときにmasterの状態をなんとか整合性をもって同期する必要がない。


#### 2.6.3 Operation Log

operation logはmasterのmetaデータの変更履歴となる。masterの唯一の永続データであるとともに、変更操作の順番を決定する。ファイルとchunkとそのバージョンは論理時間でユニークに識別できる。

operation logは非常に重要で、失うとクライアントの操作やファイルシステム全体を消失する。そのためlogをレプリケートしてローカルとリモートのディスクにフラッシュしてからクライアントにレスポンスを返す。

masterはファイルシステムの状態はoperation logをリプレイすることで復旧する。復旧を迅速にするため、logのサイズが大きくなると最新のcheckpointを読み込みその後のlogのみをリプレイすればいいようにする。


### 2.7 Consistency Model

#### 2.7.1 Guarantees by GFS

ファイルの名前空間の変更（e.g. ファイルの作成）はアトミック。masterしかこの操作を行わない。

変更後のファイル領域の状態は

- 変更の種類
- 成功か失敗か
- 並列か

に依存する

変更後のファイル領域の状態の定義

- consistent: すべてのクライアントがどのレプリカを見ても同じデータを観測する
- defined: consistentかつクライアントがすべての変更内容を観測すること

変更後のファイル領域の状態のシナリオ

- 並列書き込みが起きなかった場合：defined
- 並列書き込みが起きた場合：undefined but consistent。すべてのクライアントは同じデータを観測するが、その変更を反映しているものではないかもしれない。たいてい複数の変更が混在した結果となる。
- 変更が失敗した場合：incnsistent (hence also undefined)。異なるクライアントは異なる結果を観測する

データ変更の種類
- write：データをアプリケーションが指定したoffsetに書き込む
- record append：GFSが選んだoffsetにatomically at least onceに書き込む

一連の成功した変更のあとに、ファイル領域はdefinedであり最後の変更を含んでいることが保証される。
GFSはこれを以下の方法で実現している

- すべてのレプリカでchunkへの変更を同じ順番で行う
- chunkserverがダウンしている間変更が適用されず遅れたレプリカを判定するために、chunkのバージョン番号を使う

遅れたレプリカは変更されずゴミとして回収される。

クライアントはchunkの場所をキャッシュしているため、キャッシュをリフレッシュする前に遅れたレプリカからchunkを読んでしまうことがある。この間隔はキャッシュのタイムアウトと次のファイルopenに制限される。ほとんどのファイルは追記のみなので、遅れたレプリカは古いデータではなくchunkの早まった終了位置を返す。クライアントがリトライすればmasterから最新のchunkの場所を取得できる。

変更が成功しても障害がデータを壊すことはもちろんある。GFSは障害が起きたchunkserverをmasterとすべてのchunkserverとのハンドシェイクで特定し、チェックサムで破損を検知する。
破損したデータは正常なレプリカから直ちに修復される。GFSが反応するまでに（だいたい数分以内）すべてのレプリカが失われた時のみ、chunkが不可逆的に失われる。このとき利用不可能になるが、破損にはならない。クライアントは破損したデータではなくエラーを受け取る。


#### 2.7.2 Implications for Applications

GFSはシンプルな技術でのゆるい整合性を受け入れている。

- 上書きではなく追記
- checkpointing
- self-validating
- self-identifying

追記は効率がよく耐障害性が高い。
checkpointingではwriterがインクリメンタルに再開でき、readerがまだ書き終わってないデータを処理することを防いでくれる。


### 3. SYSTEM INTERACTIONS

#### 3.1 Leases and Mutation Order

レプリカ間の整合性のある変更を行うためにchunkのリースを使う。masterはレプリカの１つにchunkのリースを与える。そのレプリカを *primary* と呼ぶ。primaryはchunkのすべての変更への順序を選ぶ。なのでグローバルな変更順序はmasterに選ばれたリース貸与順序と、リースの中ではprimaryが与えた順番によって定義される。

masterがprimaryとの通信を失っても、古いリースが期限切れになったら他のレプリカが
リースを得る。

Figure 2で書き込みのプロセスを示す。

1. クライアントはどのchunkserverがchunkのリースをもっていて、他のレプリカがどこにいるのか聞く
2. masterはprimaryとsecondaryレプリカの場所を返す。クライアントはそれをキャッシュする。
3. クライアントはすべてのレプリカにデータをプッシュする。順番は問わない。各chunkserverはLRUバッファーキャッシュに書き込む。
4. すべてのレプリカがデータを受け取りAckを返したら、クライアントはwriteリクエストをprimaryに送る。primaryは（他のクライアントのものも含め）受け取ったすべての変更操作に順番をつける。その順番で変更を適用し内部状態を変える。
5. primaryはwriteリクエストをすべてのsecondaryレプリカにフォワードする。secondaryはprimaryが決めた順番どおりに変更を適用する。
6. secondaryは変更が完了したらprimaryに通知する
7. primaryはクライアントにレスポンスを返す。レプリカでおきたエラーはクライアントに報告される。このときsecondaryで書き込みが失敗しているかもしれないが、primaryでは成功している。なぜならprimaryで失敗したら順序が決定できずsecondaryにフォワードできないからだ。このときファイル領域はinconsistentとなり、リクエストはエラーとなる。

クライアントの操作は並列に起きるかもしれない。なので共有されたファイル領域はundefinedだが、すべてのレプリカで変更は同じ順番で起きており、結果は同一となるのでconsistentである。


#### 3.2 Data Flow

ネットワーク効率のためにコントロールフローとデータフローを分離した。

目標
1. 各マシンのネットワーク帯域をフルに使う
2. ネットワークのボトルネックを回避する
3. データをプッシュするレイテンシを最小化する

各マシンのネットワーク帯域をフルに使うために、データはchunkserverのチェーンに直列にプッシュされる。

ネットワークのボトルネックを回避してレイテンシを最小化するために、各マシンはデータを近いサーバーにフォワードする。ネットワークトポロジ上の距離はIPアドレスから推測する。

レイテンシを最小化するために、chunkserverはデータを受け取るとすぐにフォワードを開始する。


#### 3.3 Atomic Record Appends

通常のwriteでは、クライアントがデータを書きたいoffsetを指定する。同一領域への並列書き込みは順序付けられていない。なのでファイル領域は複数のデータフラグメントを持つことがある。

appendでは、クライアントはデータのみを指定する。GFSはそれを１つの連続したバイト列としてat least onceにアトミックにファイルに追記する。offsetはGFSが選択し、クライアントに返す。

appendはmutation操作の１つで、primaryでの追加ロジックを伴うほかは3.1で解説したコントロールフローに従う。クライアントはファイルの最後のchunkをすべてのレプリカにプッシュし、それからprimaryにリクエストを送信する。primaryはchunkへのappendがchunkの最大サイズを超えないかチェックする。最大サイズを超える場合、chunkを最大サイズまで埋め、すべてのsecondaryでもそうするよう指示し、クライアントに次のchunkで操作を再開するよう要求する。chunkの最大サイズに収まるなら、primaryはレプリカにデータをappendし、secondaryに正確なoffsetに書き込むよう指示し、最後にクライアントに成功を返す。

もしレプリカで失敗が起きたら、クライアントはリトライする。この結果レプリカ間の同一のchunkは重複により異なるデータとなる可能性がある。GFSはすべてのレプリカがバイトレベルで同一であることを保証しない。データがアトミックでat least onceに書き込まれることを保証するだけだ。この特徴はクライアントが成功を帰す場合データはすべてのレプリカで同じoffsetに書かれるという制約に従う。整合性の保証という観点からは、appendが成功した領域はdefined(hence consistent)で、間にある領域はinconsistent(hence undefined)になる。私達のアプリケーションは2.7.2で説明したように不整合に対処できる。

#### 3.4 Snapshot

