Designing Akka Cluster Resilience

# なぜAkka Clusterを使うのか？

AkkaやErlangを始めとするアクタープログラミングでは、位置透過性によりどのスレッドでアクターが動いても同じように動作することが保証されています。この性質によりマシンにコアを追加すればコードを変えることなくアプリケーションをスケールアップすることができます。

一方スケールアップだけでは不十分な場合、スケールアウトを考えなくてはなりません。このときも位置透過性によりアクターはどのサーバーで動いても同じように動作することが保証されています。これによってほとんどコードを変えずにスケールアウトに戦略を切り替えることができます。

位置透過性はサービスの各段階において適したスケール戦略を少ないコストで可能にします。どのようなスケール戦略をとるかによってコードを変える必要がなく、スレッドや複数マシンへの分散環境の複雑なテストを書く量も少なくて済み、すべてのチームメンバーが分散プログラミングに精通している必要がないという点で低コストです。

コアを使い切るという目的で十分な場合が多いので、分散環境でアクタープログラミングをしている人は少ないと思います。しかし位置透過性がある限りすべてのアクタープログラマーが分散環境への準備ができているといえます。分散を見越して設計し、いざ分散させたときにコードの変更量が少ないコードを書ける人は、よりすぐれたアクタープログラマーと言えます。分散環境でアクタープログラミングをしていない方たちにも今日の発表の価値を見出していただけるとうれしいです。


# 耐障害性をデザインする

すべての分散システムと同様、Akka Clusterを使ったシステムもまた耐障害性をデザインしなければなりません。

耐障害性をデザインする前に、故障の単位を定義しておきましょう。故障の単位をプロセスといいます。アクタープログラミングでは障害の単位はアクターですが、ここではAkka Clusterの１ノードと定義します。これはAkka ClusterアプリケーションのUNIXプロセスと同義になります。

//故障しているプロセスがresilience

耐障害性をデザインする際に、どのような故障に対処するのかを定義しなければなりません。故障には主に４つの分類があります。

- ビザンチン（任意）故障
 - クラッシュ･リカバリー故障
  - オミッション（切り捨て）故障
   - クラッシュストップ故障

この４つの故障はそれぞれ下位の故障の上位集合になっています。上位の故障ほど対処が難しいです。

## クラッシュストップ故障

正常に処理を実行していたプロセスがある時刻以降処理を停止して２度と戻らない故障をクラッシュストップ故障といいます。故障のなかでもっとも単純な故障です。



## オミッション（切り捨て）故障



## クラッシュ･リカバリー故障

クラッシュ･リカバリー故障はプロセスがクラッシュしただけでなく、そこからリカバリーできない、あるいはクラッシュと再起動を繰り返してしまう故障です。リカバリーできない場合、送るべきはずのメッセージを送れないので、オミッション故障と見ることもできます。

Akka Clusterのレイヤーではクラッシュしてクラスターから取り除かれたノードがクラスターに再加入することを前提としていません。なのでクラッシュしたままでも問題はなく、クラッシュ･リカバリー故障はおきません。

Akka Clusterが再起動したノードをどのように扱うかを解説します。Akka Clusterはメンバーを３つの識別子のタプルで認識します。hostname:port:uidです。uidはアクターシステム起動時に発行されるユニークなIDです。このuidによってたとえホストとポートが同じでも、再起動した後ではuidが異なります。つまり再起動してクラスターに再加入することは、新しいメンバーがクラスターに加入することと全く同じです。こうすることによってAkka Clusterはクラッシュ･リカバリー故障を考慮する必要がなく、単純化しています。

ノードがクラッシュした後再起動してクラスターに加入する最、クラッシュしたメンバーはunreachableとなってリーダーアクションがとれないため、ノードが再加入できるのか疑問が残ります。このような蘇り(incurnation)ノードの扱いについて解説します。クラスターのメンバーと同じホスト：ポートのペアをもつメンバーが加入（joining）してきた場合、Leaderは古いメンバーを自動的にdownします。これはauto-downやsplit brain resolverを使わなくても行われます。これによって再起動したノードはクラスターに参加（up）することができます。なぜこのケースでdownが安全に可能かというと、同じホスト：ポートのペアをもつメンバーが同時に２つ存在することはありえないからです。古いメンバーはクラッシュしたということをLeaderは決定的に判断できます。このような特徴があるので、基本的にプロセスを再起動することを勧めます。そうすればクラッシュしてもクラスターが縮小することはなく、自動的にもとに戻ります。

単純化のためにも、クラッシュ･リカバリー故障を考慮する必要があるアプリケーションにすることはお勧めしません。永続的な状態を管理する際は、ノードの再起動を前提にしないよう、他のメンバーが状態を肩代わりできるようにしましょう。

複数のDCにAkka Clusterをデプロイしている場合、再起動には注意が必要です。２つのAZ間でネットワーク分断を起こした場合、split brain resolverによって片方のAZのメンバーのみが生き残ります（AZ1とします）。自ら死を選んだもう一方のAZ（AZ2とします）のメンバーは再起動した後、joining状態でネットワーク分断が解消されるまで待ち続け、ネットワーク分断が解消されたら再びクラスターに戻ることが理想です。このときクラスターに加入するための第一seedノードは、生き残った方のAZ1に存在しなければなりません。さもないと再起動したAZ2のノードたちは、それらだけでクラスターを作ってしまい、split brain状態になってしまいます。第一seedノードはクラスターを０から構築する際の起点になるので、split brain resolverで残す判断の基準になるノード群（quorum/oldest/majority）があるAZに置く必要があります。プロセスの再起動を有効にして複数のAZで利用する際は、どのAZをクラスターの正とするか決め、それを固定することをお勧めします。


## ビザンチン（任意）故障





