Designing Akka Cluster Resilience

# なぜAkka Clusterを使うのか？

AkkaやErlangを始めとするアクタープログラミングでは、位置透過性によりどのスレッドでアクターが動いても同じように動作することが保証されています。この性質によりマシンにコアを追加すればコードを変えることなくアプリケーションをスケールアップすることができます。

一方スケールアップだけでは不十分な場合、スケールアウトを考えなくてはなりません。このときも位置透過性によりアクターはどのサーバーで動いても同じように動作することが保証されています。これによってほとんどコードを変えずにスケールアウトに戦略を切り替えることができます。

位置透過性はサービスの各段階において適したスケール戦略を少ないコストで可能にします。どのようなスケール戦略をとるかによってコードを変える必要がなく、スレッドや複数マシンへの分散環境の複雑なテストを書く量も少なくて済み、すべてのチームメンバーが分散プログラミングに精通している必要がないという点で低コストです。

コアを使い切るという目的で十分な場合が多いので、分散環境でアクタープログラミングをしている人は少ないと思います。しかし位置透過性がある限りすべてのアクタープログラマーが分散環境への準備ができているといえます。分散を見越して設計し、いざ分散させたときにコードの変更量が少ないコードを書ける人は、よりすぐれたアクタープログラマーと言えます。分散環境でアクタープログラミングをしていない方たちにも今日の発表が役に立つとうれしいです。


# Akka Clusterの適用領域

Akka Clusterアプリケーションはmulti-tierサービスのうちfirst-tier、つまりクライアントとのインタラクションを伴いダイナミックにスケールすることが求められる領域に適用できます。Akka Clusterアプリケーションは高可用性や高い応答性のためにsoft-stateを扱う際に有用です。


# 耐障害性をデザインする

すべての分散システムと同様、Akka Clusterを使ったシステムもまた耐障害性をデザインしなければなりません。

耐障害性をデザインする前に、故障の単位を定義しておきましょう。故障の単位をプロセスといいます。アクタープログラミングでは障害の単位はアクターですが、ここではAkka Clusterの１ノードと定義します。これはAkka ClusterアプリケーションのUNIXプロセスと同義になります。

//故障しているプロセスがresilience

耐障害性をデザインする際に、どのような故障に対処するのかを定義しなければなりません。故障には主に４つの分類があります。

- ビザンチン（任意）故障
 - クラッシュ･リカバリー故障
  - オミッション（切り捨て）故障
   - クラッシュストップ故障

この４つの故障はそれぞれ下位の故障の上位集合になっています。上位の故障ほど対処が難しいです。

## クラッシュストップ故障

正常に処理を実行していたプロセスがある時刻以降処理を停止して２度と戻らない故障をクラッシュストップ故障といいます。故障のなかでもっとも単純な故障です。後により上位の故障を説明しますが、考えなければならない故障のほぼとんどはクラッシュストップ故障です。

クラスターのメンバーの１つがクラッシュストップ故障を起こした場合、どの処理が止まるのか明らかにしましょう。

- リーダーアクションを行えなくなる（メンバーの追加ができない）
- そのノードへのメッセージ送信が失敗する
- クラスターシングルトンノードが故障した場合、故障したメンバーをクラスターから取り除かない限りフェイルオーバーが行われない

これ以外の処理は正常に動作し続けます。それらの処理によって１台のノードの故障はまったく問題はないか、あるいは部分的な障害で収まります。

Akka Clusterのレイヤーではメンバーの一部がクラッシュストップ故障を起こした場合、そのメンバーはfailure detectorによってunreachableと判定され、ゴシップは非収束状態になります。このときクラスターは保守的に振る舞い、クラスターの状態を変えるリーダーアクションを行うことをやめます。リーダーアクションを再び行えるようにするには、unreachableなメンバーがfailure detectorによって再びreachableとなるか、down状態にする必要があります。down状態にすると、その状態変化を全メンバーが観測してゴシップは収束し、やがてリーダーはそれをremoved状態にしてクラスターから取り除きます。down状態になるとメンバーの追加が可能になります。またremoved状態になるとクラスターシングルトンのフェイルオーバーがおきます。

デフォルトでは、Akka Clusterはunreachableなメンバーを自動的にdown状態にはしません。つまり人の手でdownするか、故障したプロセスを再起動（後述の蘇り(incurnation)ノード参照）する必要があります。

自動的に故障したメンバーをdownするには、auto-downというオプションを有効にします。この機能を有効にすると、unreachableなメンバーは指定時間後に自動的にdownします。しかしこの機能は使ってはいけません。なぜ使ってはならないのか、考えてみましょう。

### split brain問題

根本的な問題は、failure detectorがメンバーをunreachableと判定したとき、そのメンバーは本当に死んでいるのか、ネットワーク遅延や分断なのか、GCや負荷による遅延なのかが分からないということです。よいfailure detectorとは、あくまでノードがダウンしていることを精度良く **推定**できるだけにすぎません。この問題はすべての分散システムに言えることで、分散システムが難しい理由の１つです。

downと判定したメンバーが本当に停止しているわけではなく、生きていた場合を考えましょう。このメンバーはすでにクラスターから切り離され、状態は同期できなくなっています。このメンバーがクライアントに公開されてしまうと、古い、あるいは間違ったレプリカデータ、複数存在して互いに不整合な状態をもつシャーディングされたエンティティという誤った情報をクライアントに伝え、またその誤った状態をもとにデータベースなどの共有リソースを変更してしまいます。このようにクラスターが分裂してしまうことをsplit brain問題といいます。

auto-downの危険性をよく理解するためにはリーダーがどのように決定されるかを知っておく必要があります。なぜならauto-down機能でメンバーをdown状態にするのはリーダーだからです。Akka Clusterにはリーダー選出という過程は存在しません。リーダーはgossipプロトコルで同期されたメンバーリストから決定論的に決定できます。リーダーはunreachableでないメンバーのうちUpとLeaving状態のもので（UpやLeavingなメンバーが優先されるだけで、他の状態でもリーダーになれる）、アドレス順に並べて先頭のものです。

```scala
/*
 * 実際にはUpやLeavingなメンバーが優先的にリーダーになれるにすぎない。他の状態でもリーダーになれる。実際のコードは以下を参照。
 * https://github.com/akka/akka/blob/v2.4.10/akka-cluster/src/main/scala/akka/cluster/Gossip.scala#L190-L196
 */
reachableMembers.filter(m => m.status == Up || m.status == Leaving).map(_.address).min(Member.addressOrdering)
```

この決定方法ではunreachableなメンバーがいない場合、つまりゴシップが収束しているときにクラスター内でリーダーをユニークに決定できます。しかしunreachableなメンバーが存在すると、メンバーによってどのメンバーがunreachableに見えるのかが異なるため、メンバーによって異なるリーダーを決定する場合があります。例えばネットワーク分断によりクラスターが２つに分断された場合、リーダーは２つできます。各リーダーが相手側をdownするので、split brain状態になります。Akka Clusterのリーダー選出はどのような状況でも可能で可用性が高い代わりに、整合性を犠牲にしています。これは整合性を取っているPaxosやRaftなどの分散合意プロトコルと異なる点です。

### Split Brain Resolving Strategies

split brain問題に対処するためにはいくつかの観点があり、それにどのような答えを出すかによってsplit brain問題を解決する方法が変わります。

- クラスターの中でリーダーよりももっと整合性の高い方法で決定でき、downを果たせるメンバーはいないでしょうか？
- クラスターが２つに分割された場合、どちらが正しいクラスターなのでしょうか？
- 正しくないクラスターを決めたとして、そのクラスターのメンバーはどうすべきでしょうか？




## オミッション（切り捨て）故障

オミッション故障とは、プロセスが送るべきメッセージを送らない、あるいは受信するべきメッセージを受信できない故障です。プロセスがクラッシュした場合も送るべきメッセージを送れないので、オミッション故障はクラッシュストップ故障のより一般的な場合と見ることができます。

Akka Clusterでは、送るべきはずのシステムメッセージがあまりに送信されず、ローカルとリモートのアクターシステム間の状態が同期できなくなったときにオミッション故障となります。具体的にシステムメッセージとは、リモートスーパーバイザーに管理されたアクターのライフサイクルイベント、watchによる死活監視、リモートアクターのデプロイメントが該当します。これらの送信が確認できないときにAkka Remoteの状態はquarantinedになります。quarantinedになると、そのメンバーはunreachableから戻ってこれなくなります。解消するにはクラスターから取り除くかアクターシステムを再起動する必要があります。auto-downやsplit brain resolverを使用して適切に設定している場合、unreachableなメンバーをクラスターから取り除く作業は自動で行われ、取り除かれたメンバーはアクターシステムをシャットダウンします。つまりAkka Clusterにおいてオミッション故障はクラッシュストップ故障と全く同じ扱いになります。


## クラッシュ･リカバリー故障

クラッシュ･リカバリー故障はプロセスがクラッシュしただけでなく、そこからリカバリーできない、あるいはクラッシュと再起動を繰り返してしまう故障です。リカバリーできない場合、送るべきはずのメッセージを送れないので、オミッション故障と見ることもできます。

Akka Clusterのレイヤーではクラッシュしてクラスターから取り除かれたノードが再起動してクラスターに再加入することを前提としていません。なのでクラッシュしたままでも問題はなく、クラッシュ･リカバリー故障はおきません。

Akka Clusterが再起動したノードをどのように扱うかを解説します。Akka Clusterはメンバーを３つの識別子のタプルで認識します。hostname:port:uidです。uidはアクターシステム起動時に発行されるユニークなIDです。このuidによってたとえホストとポートが同じでも、再起動した後ではuidが異なります。つまり再起動してクラスターに再加入することは、新しいメンバーがクラスターに加入することと全く同じです。こうすることによってAkka Clusterはクラッシュ･リカバリー故障を考慮する必要がなく、単純化しています。

ノードがクラッシュした後再起動してクラスターに加入する最、クラッシュしたメンバーはunreachableとなってリーダーアクションがとれないため、ノードが再加入できるのか疑問が残ります。このような蘇り(incurnation)ノードの扱いについて解説します。クラスターのメンバーと同じホスト：ポートのペアをもつメンバーが加入（joining）してきた場合、Leaderは古いメンバーを自動的にdownします。これはauto-downやsplit brain resolverを使わなくても行われます。これによって再起動したノードはクラスターに参加（up）することができます。なぜこのケースでdownが安全に可能かというと、同じホスト：ポートのペアをもつメンバーが同時に２つ存在することはありえないからです。古いメンバーはクラッシュしたということをLeaderは決定的に判断できます。このような特徴があるので、基本的にプロセスを再起動することを勧めます。そうすればクラッシュしてもクラスターが縮小することはなく、自動的にもとに戻ります。

単純化のためにも、クラッシュ･リカバリー故障を考慮する必要があるアプリケーションにすることはお勧めしません。永続的な状態を管理する際は、ノードの再起動を前提にしないよう、他のメンバーが状態を肩代わりできるようにしましょう。

複数のDCにAkka Clusterをデプロイしている場合、再起動には注意が必要です。２つのAZ間でネットワーク分断を起こした場合、split brain resolverによって片方のAZのメンバーのみが生き残ります（AZ1とします）。自ら死を選んだもう一方のAZ（AZ2とします）のメンバーは再起動した後、joining状態でネットワーク分断が解消されるまで待ち続け、ネットワーク分断が解消されたら再びクラスターに戻ることが理想です。このときクラスターに加入するための第一seedノードは、生き残った方のAZ1に存在しなければなりません。さもないと再起動したAZ2のノードたちは、それらだけでクラスターを作ってしまい、split brain状態になってしまいます。第一seedノードはクラスターを０から構築する際の起点になるので、split brain resolverで残す判断の基準になるノード群（quorum/oldest/majority）があるAZに置く必要があります。プロセスの再起動を有効にして複数のAZで利用する際は、どのAZをクラスターの正とするか決め、それを固定することをお勧めします。


## ビザンチン（任意）故障

プロセスが想定と異なる検知可能な挙動を行う故障をビザンチン故障と呼びます。



